package valuePolicyIterationPack;

import java.util.Vector;

import actionPack.RandomAction;
import agentsPack.Predator;
import agentsPack.Prey;
import environmentPack.Coordinate;
import environmentPack.Environment;

public class ValueIteration {

	public static void main(String[] args) {

		ValueIterationImpl(new Coordinate(5, 5), 0.7);
		ValueIterationImplSw(new Coordinate(5,5),0.7);

	}

	public static void ValuesMirroring(double[][] SmallWorld, Coordinate Prey) {
		
		PrintValueIteration(SmallWorld);
		double[][] State = new double[11][11];
		
		int currentPosX = Prey.getX();
		int currentPosY = Prey.getY();
		int xCur=-1;
		int yCur=-1;
		for(int x=Prey.getX();x < (Prey.getX()+6);x++){
			currentPosX = (x + 11) % 11;
			xCur++;
			yCur = -1;
			for(int y=Prey.getY();y < (Prey.getY()+6);y++){
				yCur++;
				currentPosY = (y + 11) % 11;
				State[currentPosX][currentPosY] = SmallWorld[xCur][yCur];
			}
			
		}
		
		xCur=-1;
		for(int x=Prey.getX();x > (Prey.getX()-6);x--){
			currentPosX = (x + 11) % 11;
			xCur++;
			yCur = -1;
			for(int y=Prey.getY();y < (Prey.getY()+6);y++){
				yCur++;
				currentPosY = (y + 11) % 11;
				State[currentPosX][currentPosY] = SmallWorld[xCur][yCur];
			}
			
		}
		
		xCur=-1;
		for(int x=Prey.getX();x > (Prey.getX()-6);x--){
			currentPosX = (x + 11) % 11;
			xCur++;
			yCur = -1;
			for(int y=Prey.getY();y > (Prey.getY()-6);y--){
				yCur++;
				currentPosY = (y + 11) % 11;
				State[currentPosX][currentPosY] = SmallWorld[xCur][yCur];
			}
			
		}
		
		xCur=-1;
		for(int x=Prey.getX();x < (Prey.getX()+6);x++){
			xCur++;
			yCur = -1;
			currentPosX = (x + 11) % 11;
			for(int y=Prey.getY();y > (Prey.getY()-6);y--){
				yCur++;
				currentPosY = (y + 11) % 11;
				State[currentPosX][currentPosY] = SmallWorld[xCur][yCur];
			}
			
		}
		
		PrintValueIteration(State);
		
		
		
		
		
		
		
	}

	public static void ValueIterationImplSw(Coordinate PreyPosition, double discountFactor) {
		
		long start = System.currentTimeMillis();

		double[][] State = new double[6][6];
		double delta;
		double preValue;
		int algorithmSweeps = 0;

		do {

			delta = 0;
			algorithmSweeps++;
			for (int i = 0; i < State.length; i++) {
				for (int j = 0; j < State.length; j++) {

					//Previous values of each state
					preValue = State[i][j];

					//Generation of the new environment
					Environment env = new Environment();
					Predator P = new Predator("Predator", new Coordinate(i, j),null);
					Prey p = new Prey("prey", new Coordinate(0, 0), null);
					//Prey and Predator add to the worldstate
					env.addAgent(P);
					env.addAgent(p);

					//Possible actions of the Predator along with probability of each action
					Vector<RandomAction> PredatorActions = P
							.ProbabilityActionsSW(env.worldState);
					//Possible actions of the Prey along with probability of each action
					Vector<RandomAction> PreyActions = p
							.ProbabilityActionsSW(env.worldState);

					// State in which prey and predator are located in the same
					// position,
					// default reward value is given to this state
					if (rewardFunction(P.position, p.position) > 0) {

						State[i][j] = 0;

					} else {

						double maxCurValue = 0;

						// Exploring through all posible states, generated by
						// the
						// actions from both predator and prey
						for (int ii = 0; ii < PredatorActions.size(); ii++) {

							double ValueCurrent = 0;

							for (int jj = 0; jj < PreyActions.size(); jj++) {

								// Probability of each state
								double stateProbability = 
										PreyActions
										.elementAt(jj).prob;

								// Reward of each state
								double reward = rewardFunction(
										PredatorActions.elementAt(ii).coordinate,
										PreyActions.elementAt(jj).coordinate);

								if (Coordinate
										.compareCoordinates(
												PredatorActions.elementAt(ii).coordinate,
												p.position)) {
									reward = 10;
								}

								// Previous values multiplied by the discount
								// factor
								double discount = discountFactor
										* State[PredatorActions.elementAt(ii).coordinate
												.getX()][PredatorActions
												.elementAt(ii).coordinate
												.getY()];

								ValueCurrent += stateProbability
										* (reward + discount);

							}

							// pair of actions with maximum value
							maxCurValue = Math.max(maxCurValue, ValueCurrent);

						}

						State[i][j] = maxCurValue;
						delta = Math.max(delta,
								Math.abs(maxCurValue - preValue));

					}

				}

			}

		} while (delta > Math.pow(10, -3));
		
		
		long end = System.currentTimeMillis();
		
		System.out.println("\nSmall 6x6 World Implementation");
		System.out.println("\nSweeps = " + algorithmSweeps);
		System.out.println("Execution time was " + (end-start) + "ms");
		
		ValuesMirroring(State , PreyPosition);
		
		
	}

	public static void ValueIterationImpl(Coordinate PreyPosition,
			double discountFactor) {

		long start = System.currentTimeMillis();

		double[][] State = new double[11][11];
		double delta;
		double preValue;
		int algorithmSweeps = 0;

		do {

			delta = 0;
			algorithmSweeps++;
			for (int i = 0; i < State.length; i++) {
				for (int j = 0; j < State.length; j++) {

					// Previous values of each state
					preValue = State[i][j];

					// Generation of the new environment
					Environment env = new Environment();
					Predator P = new Predator("Predator", new Coordinate(i, j),
							null);
					Prey p = new Prey("prey", PreyPosition, null);
					// Prey and Predator add to the worldstate
					env.addAgent(P);
					env.addAgent(p);

					// Possible actions of the Predator along with probability
					// of each action
					Vector<RandomAction> PredatorActions = P
							.ProbabilityActions(env.worldState);
					// Possible actions of the Prey along with probability of
					// each action
					Vector<RandomAction> PreyActions = p
							.ProbabilityActions(env.worldState);

					// State in which prey and predator are located in the same
					// position,
					// default reward value is given to this state
					if (rewardFunction(P.position, p.position) > 0) {

						State[i][j] = 0;

					} else {

						double maxCurValue = 0;

						// Exploring through all posible states, generated by
						// the
						// actions from both predator and prey
						for (int ii = 0; ii < PredatorActions.size(); ii++) {

							double ValueCurrent = 0;

							for (int jj = 0; jj < PreyActions.size(); jj++) {

								// Probability of each state
								double stateProbability = 
										PreyActions
										.elementAt(jj).prob;

								// Reward of each state
								double reward = rewardFunction(
										PredatorActions.elementAt(ii).coordinate,
										PreyActions.elementAt(jj).coordinate);

								if (Coordinate
										.compareCoordinates(
												PredatorActions.elementAt(ii).coordinate,
												p.position)) {
									reward = 10;
								}

								// Previous values multiplied by the discount
								// factor
								double discount = discountFactor
										* State[PredatorActions.elementAt(ii).coordinate
												.getX()][PredatorActions
												.elementAt(ii).coordinate
												.getY()];

								ValueCurrent += stateProbability
										* (reward + discount);

							}

							// pair of actions with maximum value
							maxCurValue = Math.max(maxCurValue, ValueCurrent);

						}

						State[i][j] = maxCurValue;
						delta = Math.max(delta,
								Math.abs(maxCurValue - preValue));

					}

				}

			}

		} while (delta > Math.pow(10, -5));

		long end = System.currentTimeMillis();
		System.out.println("Normal 11x11 World Implementation");
		System.out.println("\nSweeps = " + algorithmSweeps);
		System.out.println("Execution time was " + (end - start) + "ms");
		PrintValueIteration(State);

	}

	public static double rewardFunction(Coordinate a1, Coordinate a2) {

		if ((Coordinate.compareCoordinates(a1, a2))) {
			return 10;
		}

		return 0;

	}

	public static void PrintValueIteration(double[][] StateValues) {
		System.out.println();
		for (int i = 0; i < StateValues.length; i++) {
			System.out.println();
			for (int j = 0; j < StateValues.length; j++) {
				System.out.printf(" %.4f |", StateValues[i][j]);
			}
		}

	}

}