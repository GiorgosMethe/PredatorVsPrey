\documentclass[a4paper,11pt]{article}

\usepackage{fullpage}
\usepackage[usenames,dvipsnames]{color}
\usepackage{hyperref}
%\usepackage{tabularx}
%\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{multirow}
%\usepackage{layouts}
%\usepackage{array}
%\usepackage{pgf}
%\usepackage{tikz}
%\usetikzlibrary{positioning}
%\usetikzlibrary{arrows,automata}
\usepackage{graphicx}

\hypersetup{
  colorlinks,%
    citecolor=black,%
    filecolor=black,%
    linkcolor=black,%
    urlcolor=mygreylink     % can put red here to visualize the links
}

\newcommand{\Pred}[2]{\ensuremath{\mathtt{Predator}\left<#1, #2\right>}}
\newcommand{\Prey}[2]{\ensuremath{\mathtt{Prey}\left<#1, #2\right>}}

\definecolor{mygrey}{gray}{.85}
\definecolor{mygreylink}{gray}{.30}
\textheight=8.6in
\raggedbottom
\raggedright
\setlength{\tabcolsep}{0in}
\addtolength{\oddsidemargin}{-0.375in}
\addtolength{\evensidemargin}{0.375in}
\addtolength{\textwidth}{0.5in}
\addtolength{\topmargin}{-.375in}
\addtolength{\textheight}{0.75in}

\newcommand{\resheading}[1]{{\large \colorbox{mygrey}{\begin{minipage}{\textwidth}{\textbf{#1 \vphantom{p\^{E}}}}\end{minipage}}}}

\newcommand{\mywebheader}{
  \begin{tabular}{@{}p{5in}p{4in}}
  {\resheading{Assignment 1: Single Agent Planning}} & {\Large 21 September, 2012}\\\vspace{0.2cm}
  \end{tabular}}

\begin{document}

\begin{center}
{\LARGE \textbf{Autonomous Agents}}\\ [1em]
\end{center}
\mywebheader

\begin{center}
{\Large By:} \\ \vspace{0.1cm}
{\Large Paris Mavromoustakos} \\  \vspace{0.1cm}
{\Large Georgios Methenitis} \\ \vspace{0.1cm}
{\Large Patrick de Kok} \\ \vspace{0.1cm}
{\Large Marios Tzakris}
\end{center}


\section*{Introduction}
The purpose of this first assignment was to implement a reinforcement learning task that satisfies the Markov property, known as Markov Decision Process (MDP). To achieve this we used one prey, which is part of the environment and behaves in a known probabilistic way, and one agent, the predator, whose goal is to catch the prey. When that happens the episode ends. The environment we used is a 11 by 11 toroidal grid. Also we assumed that the entire MDP is known to our agent. The agent could thus determine the optimal policy even before interacting with the environment. To program the assignment we used the programming language Java.

\section*{Exercise 1}
In the first part, we simulated the environment keeping the policies of the predator and the prey separate. The predator starts from position $\left<0,0\right>$ and the prey from $\left<5,5\right>$ moving randomly and depending on the given probabilities. The time it takes on average for the predator to catch the prey with random policy is % TODO: Include average step count over 100 runs
, and the standard deviation is % TODO: Include stdev over 100 runs
.

% TODO: Make a simple command line user interface to the code, and tell about it.
% >> What do you want to do?
% >>     1) Run the random simulation for n times
% >>     2) Run ValueIteration
% >>     ...



\section*{Exercise 2}
In order to evaluate the random policy we computed the state-value function $V^{\pi_{random}}$ and determined the following values for the given states:\\

\begin{align*}
V^{\pi_{random}}(\left\{\Pred{0}{0},\Prey{5}{5}\right\}) & = \ldots \\
&= bla \\
&= blabla\\
    V^{\pi_{random}}(\left\{\Pred{2}{3},\Prey{5}{4}\right\}) &= \ldots \\
    V^{\pi_{random}}(\left\{\Pred{2}{10},\Prey{10}{0}\right\}) &= \ldots \\
    V^{\pi_{random}}(\left\{\Pred{10}{10},\Prey{0}{0}\right\}) &= \ldots
\end{align*}

It takes % TODO: Compute sweeps to converge
sweeps to converge.



\section*{Exercise 3}
mpla mpla mpla mpla god bless you Diederik

\section*{Exercise 4}
To find an optimal policy we computed the optimal value function $V^\ast$ for all states in which the prey is located at $\left<5,5\right>$.

The convergence speed (in number of iterations) for different discount factors $\gamma$ is presented in \autoref{tab:convVI}.
\begin{table}
\caption{Trololol.}
\label{tab:convVI}
\begin{center}
\begin{tabular}{|@{ }r@{ }|@{ }r@{ }|}
\hline
Discount factor $\gamma$ & Number of iterations \\
           \hline
0.1 & \\
0.5 & \\
0.7 & \\
0.9 & \\
\hline
\end{tabular}
\end{center}
\end{table}

\section*{Exercise 5}
To iteratively improve the policy we implemented policy iteration $V^{\pi^\prime}$ for all states which the prey is located at $\left<5,5\right>$.

The convergence speed (in number of iterations) for different discount factors $\gamma$ is presented in \autoref{tab:convPI}.
\begin{table}
\caption{Trololol.}
\label{tab:convPI}
\begin{center}
\begin{tabular}{|@{ }r@{ }|@{ }r@{ }|}
\hline
Discount factor $\gamma$ & Number of iterations \\
\hline
0.1 & \\
0.5 & \\
0.7 & \\
0.9 & \\
\hline
\end{tabular}
\end{center}
\end{table}

% TODO: COMPARE THE RESULTS


\end{document}

